### 自我介绍

面试官好，非常有幸参加本次面试，我是陈博宇，一名哈尔滨工程大学研三在读的学生，接下来我先做简单自我介绍，在专业技能上面，我学习了c++编程语言、数据结构与算法、mysql和redis数据库、linux环境下网络编程等相关知识，并且了解c++常用新特性和标准模板库等相关内容。为了理论和实践相结合，目前我做了三个项目，第一个是一个基于linux环境下的的http服务器，另一个是为了学习rpc，实现了一个分布式的rpc框架。最后就是mit那个分布式系统的课程，跟着做实验的，raft合mapreduce差不多做完的时候收到一个小厂的实习就去实习了，做的区块链相关的东西，实习了一个月，5月份收到腾讯的实习就去腾讯了，期间做的向量化项目相关的工作，谢谢，我的自我介绍完毕

### 实习经历

#### 腾讯实习介绍
腾讯大数据后台开发实习，大约是五月中旬入职实习的，从上一个公司离职后直接去了，刚开始是在平台内核组，后面组织结构变动，换到了智能湖仓研发组，一直负责公司天穹os向量化项目，这里我先介绍**项目背景**就是随着网络io的性能和磁盘读写的性能在不断的提升，cpu的性能已经逐渐成为了大数据计算的性能瓶颈，大数据的计算高度依赖于jvm，然而由于jvm的gc和偏向锁是难以逾越的性能鸿沟，虽然java社区新版本已经有了向量化操作的api，但是根据测试结果来看，性能并没有很大地突破，这也就导致各个厂商在寻找解决方法，也就有了native向量化引擎的需求，我在腾讯实习，他们的向量化方案就是使用spark+gluten+velox，我简单介绍一下这个**项目的架构**，spark是大型的分布式计算框架，gluten属于中间层粘合剂，来将spark的计算任务卸载到native引擎，velox是完全使用cpp写的native向量化引擎加速库项目，采用这个方案来把计算任务卸载到native执行，来实现向量化，提升计算性能，我的**工作内容**就是开发一些合spark语义不一样的thive的udf，来避免项目上线遇见不能解析的udf产生fallbak造成很多的性能损失

**业界其他的方案** databricks photon不开源。只有论文。  datafusion blaze使用自己的protonuf来描述算子不是使用的substrait，容易与社区脱节，不易维护。

#### 可能的问题
##### 什么是偏向锁
如果一个线程获得了锁，并且在接下来的时间内没有其他线程竞争该锁，那么锁将一直偏向于该线程，这样可以避免线程每次进入同步代码块时都需要执行锁的获取和释放操作，从而减少了不必要的CAS（Compare-And-Swap）操作，提高了性能。

##### 偏向锁的性能损失
偏向锁的设计初衷是为了在低竞争的情况下提高性能，但在某些情况下，它可能会导致性能损失，主要原因有以下几点：

**撤销偏向锁的开销：**当另一个线程尝试获取偏向锁时，偏向锁需要被撤销。撤销偏向锁是一个相对复杂的过程，因为它需要将锁升级为轻量级锁或重量级锁。这一过程涉及到停止当前持有偏向锁的线程、重新分配锁状态等操作，这会带来性能开销。
**线程切换频繁：**在高竞争的场景下，偏向锁频繁地被撤销和重新获取，这不仅没有减少同步开销，反而因为频繁的锁撤销而增加了额外的开销。在这种情况下，偏向锁反而可能成为性能瓶颈。
**适应性不强：**偏向锁假设大部分情况下锁是由同一个线程持有的，但在实际应用中，如果锁的持有者频繁变化，偏向锁就不再适用，反而可能拖累性能。

##### fallback性能损失的原因
spark原生实现较native向量化引擎肯定有损失
spark和向量化引擎处理数据的方式不一样需要转换

#### 腾讯实习遇见的困难：
1. 大厂的实习体验和小厂还是不一样的，在小厂的时候，一共没几个人，我那个导师也相对重视我，基本上学习路线都给我说的很清除，按照他的路线去学就完全ok了，但是在腾讯不是，大家都很忙，基本上没有人管我，也没有比较清晰的学习路线，基本都是我自己找资料去学，完全靠自驱性，要不然工作进展不下去，而且我们正在做的工作文档很少，写的也不详细，这样就导致我第一周纯纯踩坑，根据文档配环境配了好久，踩了很多坑，最后导师告诉我代码分支搞错了，但是文档就是那个分支，重新来了，让我很苦恼，吃一堑长一智吧，后续有啥问题，我决定不了，就去问，前辈不会主动问我的，基本上都是我主动去问，但是他们会很有耐心的给我讲解，教我怎么解决

2. 大数据这个方向我基本是一点都没有接触过，因此上手难度其实还是挺大的，当时小组组长告诉我我的工作内容并给我分配任务的时候，我基本属于懵逼状态，我也很苦恼，害怕自己不能处理工作内容，处于被动状态，而且小组中前辈都比较忙，基本我不去问都不会有人来指导我的，我感觉这也就是工作的状态吧，我也适应了这个环境，我就自己找内部文档和社区文档，先把项目背景和项目框架搞清楚，然后针对自己的工作内容，寻找各种代码demo，内部的社区的都找，熟悉开发过程后再做后续的工作，在腾讯实习了三个多月，基本上一个大模块全部交给我做了。

#### 游码量化
##### 实习介绍
游码科技有限公司，当时招聘是按照量化开发工程师招的，当时面试官告诉我进去可能要做低延迟交易系统的开发，但是毕竟不是很大的公司，也就20左右个人，我工作的时候，有经验的交易系统开发的前辈还没来北京，我就先跟着导师学习链上的相关知识，这里从比特币开始了解区块链的知识，然后开始学习solana相关的知识，当时把solana加速交易的七大特性基本掌握了，了解了solana为何交易可以这么快速，掌握了solana链上程序的开发和测试环境，了解了solana链上程序的开发，当时赶上比特币奖励减半符文上线，老板让我去学习了符文相关的知识，搭建比特币全节点，然后使用go语言开发了符文的自动蚀刻工具，主要分为三个部分，首先是获取区块链mempool中的交易信息的gas费，推算出合适的gas费，然后发起蚀刻符文的交易。然后继续监控mempool中交易的gas费，适当的加速交易，避免交易被淹没损失交易费，一个月我差不多就做了这些事情，肯定有所收获，比如我去公司前都不知道区块链的东西，完全不了解web3，我这段实习经历让我了解到了web3领域，了解了去中心化的发展前景，虽然我们国家对其的政策可能没有这么自由，但是我仍然觉得这是以后不可阻挡的趋势，我后面肯定会持续关注web3相关的动态，自己可能也会涉足web3领域。

##### 游码量化实习难点：

首先，完全新的领域，在没有去这里实习的之前，我对区块链和web3一点都不了解，过去之后对未知的知识多少有点力不从心，不过还好遇见了一个不错的导师，逻辑性很强，很适合当老师，基本上我按照他给我的学习路线去查找资料学习就可以了。不过查找学习资料的过程中很难，因为东西很新，资料很少，所以在学习过程中多少会有点困难，不过请教导师后，基本上知道在哪解决疑问了，首先去官网，官网没有可以看有没有discode，再不行就去推特，我记得很清晰的就是当时比特币奖励减半，符文上线的时候，我搭建比特币全节点所遇见的问题，怎么都解决不了，最后在discord上找到了解决答案。因为实习时间不长，给我的需求不多，因此遇见的困难也不多。


### 项目

#### web server介绍

项目背景是向实践一下自己学的知识，并且熟悉一下linux环境下网络编程的模式和一些组件的编写，首先是Linux下i/o多路复用epoll，来做io事件触发，使用有限状态机来解析http的请求报文，来实现对get请求和post请求的响应，事件处理采用了Reactor模式和模拟的Proactor模式，采用线程池来实现客户端的并发访问，对一些不活跃连接使用定时器进行优化，实现了日志模块。我对项目进行了性能优化，首先使用了双缓冲的异步日志来实现日志的高性能写，内存池来避免频繁申请和释放内存导致的性能损耗和内存碎片，实现lfu缓存来对热点文件的缓存，挺高响应速度，精细化hash表的锁粒度来提升查询的并发量，实现了时间轮和时间堆定时器来优化定时器，使用cpp11标准实现了一个线程池等。对于项目的压力测试，本地测试（proactor），我使用webbench模拟10000个客户端访问5秒，结果显示未优化前qps是5k+，优化完后qps是15k+。

##### web server项目难点

- 在写阻塞队列的时候，遇见了一个卡死的现象，应该也不能叫做死锁，当时情况就是程序卡在了阻塞队列的析构函数中了，一直析构不了，当时这个问题折磨了我一晚上，最后我用gdb进行调试，分析调用栈，发现是阻塞到了条件变量，它没有接受到唤醒，析构不了，只能一直在等待唤醒。最后在阻塞队列析构的时候，先唤醒条件变量。
- 还有就是项目做完之后，做压测的时候，发现吞吐量一直上不去，一直很低，但是cpu的使用率也很高，一直找不到问题所在，然后我学习了一下火焰图的绘画和分析，对程序进行性能瓶颈分析，发现在http解析的时候非常耗时，然后我就进行分析，查资料，发现我用正则表达式是最大的错误更改回了字符串解析了，通过这个分析，我发现火焰图是个好东西啊，我使用它不断优化项目性能啊，分别是日志优化，内存池优化，lfu缓存优化和高性能hash表优化
- 找到一个bug，也不能说是bug，就是由于lfu缓存的出现，我在后台更改了文件，并不会影响到已经缓存过的内容，也就会出现既是我更改了文件，前端访问仍然是历史的版本， 但是本项目的前端并没有提供修改文件的内容的功能，因此不能算是bug，但是我还是了解了一下这个问题，就是关于缓存一致性的问题，这里我了解了一些redis做缓存时用到的缓存一致性方案，比如先删除缓存，再更新数据库（延迟双删）、先更新数据库再删除缓存（给缓存加上过期时间，针对操作失败的情况，引入消息队列），先更新数据库再更新缓存（分布式锁使得更新数据库和缓存原子执行，给缓存加上过期时间）0000，。

##### web server可能的相关问题

1. 三种定时器的区别
   - 主要的区别就是对高并发场景的影响，在多线程情况下，双有序链表每次增加定时器或者给定时器延长时间需要调整链表，因此需要对整个链表进行加锁，调整的时间复杂度是O(n)，时间堆也需要对整个堆进行上锁，但是其调整的时间复杂度是O(logn)，时间轮定时器调整的时候不需要对整个时间轮进行上锁，仅需要锁住涉及到的槽位即可，可以精细化锁粒度。
2. lfu和lru
   - lfu是根据缓存的访问频度来进行缓存置换的，访问一次缓存就增加其频度，优先置换低频度缓存（双hash记录key~频度和key~节点的映射，频度节点指向同一频度的双向链表节点，访问一次就移动到更高频度的链表）
   - lru是根据最近访问时间来进行缓存置换的，访问缓存后就将其放置到链表的头部，不是最近使用的就慢慢的调整到了尾部，优先置换尾部（hash表记录key~节点映射）
   - 对于时间相关度较低（页面访问完全是随机，与时间无关） `WebServer` 来说，经常被访问的页面在下一次有更大的可能被访问，此时使用LFU更好，并且LFU能够避免周期性或者偶发性的操作导致缓存命中率下降的问题；而对于时间相关度较高（某些页面在特定时间段访问量较大，而在整体来看频率较低）的 `WebServer` 来说，在特定的时间段内，最近访问的页面在下一次有更大的可能被访问，此时使用LRU更好。目前webserver时间相关性较低,时间相关度较高的，比如用户的token session等，用户一般在一段时间内来访问，因此使用lru比较好
   - 缺点：这样就会产生缓存污染，使得新数据块被淘汰。换句话说就是，最近加入的数据因为起始的频率很低，容易被淘汰，而早期的热点数据会一直占据缓存。  对热点数据的访问会导致 `freq` 一直递增，我目前使用 `int` 表示 `freq_` ，实际上会有溢出的风险。
   - lfu aging（条件老化）优化 在LFU算法之上，引入访问次数平均值概念，当平均值大于最大平均值限制时，将所有节点的访问次数减去最大平均值限制的一半或一个固定值。相当于热点数据“老化”了，这样可以避免频度溢出，也能缓解缓存污染   windows-lfu 每一段时间将访问次数做衰减
   - redis中的实现方式
     - lru没有采用上面描述的（用链表管理缓存占用内存空间，链表操作费时），是对结构体对象中加了一个时间字段，来记录最近访问的时间，汰换的时候随机选取一定数量的key，删除最久未访问的key
     - lfu是利用上面的时间字段的高16位记录上次访问时间，低8位来记录频率，这个低八位会根据高16位进行衰减，而实现真正的频率，不是简单的访问次数。
   - mysql 解决预读失效，缓存污染lru
     - young区和old区（7：3）预读缓存先到old区（解决预读失效），在old区中的缓存第二次访问距离第一次访问时间超过1s才挪到young区（解决缓存污染），停留一秒的原因应该就是避免大量的遍历导致缓存污染
     - linux的解决方法和mysql相似，linux分为了两个链表，old链表第二次访问就放到了young链表
3. 内存池
   - 这里使用类似stl中的二级内存分配子一样管理内存，首先根据8 16 32 ... 512的大小分为64个槽，每个槽中对应不同大小的内存池，当分配的内存小于512时，去内存池中对应的槽中取内存（先看看freelist中是否有内存，如果有就直接用，没有就去block中分配），当block中内存全部使用完后，就使用new分配4096内存，继续使用，大于512的直接使用new进行分配。释放的时候直接放入freelist中。
   - malloc自己好像是有内存池的
   - 使用`malloc/new`申请分配堆内存时系统需要根据最先匹配、最优匹配或其它算法在内存空闲块表中查找一块空闲内存；使用`free/delete`释放堆内存时，系统可能需要合并空闲内存块，会产生额外开销
   -  频繁使用时会产生大量内存碎片，从而降低程序运行效率
4. 高并发hash表
   - 这里自己实现hashmap，加锁时不再锁住整个hashmap，而是锁住需要操作的hash桶，使用shared_mutex 读操作共享，添加删除操作互斥，精细化了锁粒度。（vector+list）
5. 双缓冲日志
   - 这里分为前端和后端，前端和后端分别由两个缓冲区和一个缓冲队列组成，当数据落盘的时候，后端的缓冲区、缓冲队列会先和前端的进行交换，然后释放锁，后端的写磁盘不会影响前端的日志添加。当后端写的速率较小，那么前端缓冲区写满后，会申请新的缓冲区。
6. 观察者模式和reactor模型
   - 观察者模式的观察者需要直接向通知者注册，当通知者有事件时会直接和观察者进行通信，他两个直接交互，reactor模式不需要注册，且两者不是直接通信的，当有事件到来时，主线程会把任务压入到队列，等待线程池来消耗，也就是属于观察者不是和通知者直接通信的，而是有中间代理，这个中间代理就是阻塞队列
7. 主从状态机
   - 主状态机有三种状态分别对应请求行的解析，请求头的解析和请求内容的解析，每个主状态机状态又分为三个从状态机状态，也就是对每一行进行解析的状态，分别对应一行已经解析完毕，行中有错误字段，行数据不完整。
8. 内存模型：c++中内存模型可以分为基础结构和并发，基础结构就是c++对象的内存布局，并发就是多线程下对内存访问的次序，c++中有6中内存次序，分为三种内存模式，首先是位松散的宽松次序，也就是任何线程看到的内存改动序列可能都不相同，完全没有关系可言，例子，并发程度是最高的，然后最严格的是先后一致次序，也就是所有线程看到的内存改动序列都是一样的，然后还有就是获取释放次序，获取释放次序使得数据操作构成同步关系，一般和先行关系一块使用来达到先行的效果。



#### mprpc项目

这个项目是为了了解一下rpc的原理，借助protobuf实现rpc的功能，并且使用protobuf实现数据的序列化和反序列化，底层的网络数据传输使用了muduo网络库，服务注册和服务发现使用了zookeeper，实现了一个rpc框架，这里使用框架时，客户端需要使用protobuf生成rpc的一些必要的类，rpc服务类，和rpc客户类，当用户使用rpc客户类调用方法时，框架就会把rpc调用的参数序列化，然后通过网络传输发送到服务端，服务端对数据进行反序列化，得到方法的参数，然后调用服务端的本地方法，设置响应数据，然后再通过框架对响应数据进行序列化，通过网络发送给客户端，客户端对数据进行反序列化得到响应数据，这些数据的序列化、网络传输、反序列化都是框架在做的事情，对用户透明，使得用户就像调用本地方法一样。

框架主要有下面几个核心类
解析读取zk配置信息的类
channel类主要负责数据的序列化和底层的网络发送和接受
还有一个就是服务的注册，把用户实现的服务信息注册到zk中

##### 使用
protobuf实现rpc的介绍，使用protobuf构建rpc需要使用下面的格式，也就是声明一个service，里面加上rpc关键字，然后protobuf就会给我们生成主要的框架，其中主要包含两个类FriendServiceRpc、FriendServiceRpc_sub，其中FriendServiceRpc就是rpc服务端的类，其中有一个固定的方法CallMethod，这个方法会根据method参数来调用不同的我们自定义的接口，比如下面的GetFriendsList，一般在服务端接受到请求时，处理完请求，再调用CallMethod，来实现调用不同的方法

然后客户端是生成的FriendServiceRpc_sub类，有我们自定义的方法GetFriendsList，还有一个数据成员channel\_（需要我们自己实现，主要是用来网络通信的）当我们客户端调用GetFriendsList时，会转换成调用channel\_成员的CallMethod，来实现网络通信，也就实现了rpc  大概就是这么个原理

```cpp
service FriendServiceRpc 
{
  rpc GetFriendsList(GetFriendsListRequest) returns(GetFriendsListResponse);
}
```


#### raft项目
这个是mit分布式系统课程所做的项目，当时没有完全做完，收到一个小厂的实习就去实习了，抽时间做完了属于，主要就是了解了一下mapreduce合raft，发现还挺有用的，后面去腾讯大数据才发现mapreduce就是大数据的基石，spark没出来之前hadoop用的就是mapreduce，属于大数据的鼻祖了，然后raft协议也基本了解了，这个课还挺有用的，后续有时间把这个课跟完。


### 可能的面试题

- 分布式锁实现：简单实现set nx expire   改进redis的**redlock**
  - zookeeper实现分布式锁，采用临时有序节点来实现，当客户端需要加锁的时候向zookeeper申请一个临时有序节点，申请完之后，查看自己的节点是不是第一个节点，如果是第一个节点，那么就上锁成功，如果不是，说明已经有其他客户端上锁成功了，此时zookeeper采用watch命令进行监听比自己小的节点删除事件，如果删除，那么通知客户端上锁成功。

- raft协议：看b站视频，讲的不错
- CAP理论![image-20240131161215079](C:\Users\root\AppData\Roaming\Typora\typora-user-images\image-20240131161215079.png)
- Ceph  [Ceph的存储原理_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Wm4y1V74n/?spm_id_from=333.337.search-card.all.click&vd_source=3aa535caf6a287f00d045ee5ffb0228d)  会把文件分片成**对象存储**（osd），均匀分布到每个节点的每个硬盘，也就是一个文件的存储也可以负载均衡到不同节点，fastDFS不可以。因此Ceph在大文件存储上很有优势，fastDFS在小文件存储有很大优势（尤其是trunk策略）
- 一致性hash：将hash值组织成一个抽象的环，hash环，节点在环上，数据映射到环上，顺时针存到节点中，引入虚拟节点避免数据分布不均
- java垃圾回收，可达性算法，根据可达性算法来判断对象是否还会被使用



### raft

#### 什么是raft算法

Raft算法是一种共识算法(做冗余实现高可用的，而不是扩展)，用于在分布式系统中实现一致性。它是由Diego Ongaro和John Ousterhout于2013年提出的，旨在提供一种更易理解和可靠的分布式一致性算法。

Raft算法解决了分布式系统中的领导者选举、日志复制和安全性等关键问题。它将分布式系统中的节点划分为 **领导者（leader）、跟随者（follower）和候选者（candidate）** 三种角色，并通过一个选举过程来选择领导者。

在Raft算法中，领导者负责接收客户端的请求，并将请求复制到其他节点的日志中。跟随者和候选者则通过与领导者保持心跳和选举的方式来保持一致性。如果领导者失去联系或无法正常工作，系统会触发新一轮的选举过程，选择新的领导者。

Raft算法的设计目标是可理解性和可靠性。相比于其他共识算法如Paxos，Raft算法更加直观和易于理解，使得开发人员能够更容易地实现和调试分布式系统。

#### raft优缺点

Raft算法作为一种共识算法，在分布式系统中具有一些优点和缺点。

**优点：**

1. **简单易懂**：相比于其他共识算法，Raft算法的设计更加直观和易于理解，使得开发人员能够更容易地实现和调试分布式系统。
2. **安全性**：Raft算法保证了系统的安全性，通过领导者选举和日志复制等机制来确保数据的一致性和可靠性。
3. **高可用性**：Raft算法能够在领导者失效时快速进行新的领导者选举，从而保证系统的高可用性。

**缺点：**

1. **性能开销**：Raft算法对于每个写操作都需要进行日志复制，这会带来一定的性能开销。相比于其他共识算法如Paxos，Raft算法的性能可能会稍差一些。
2. **领导者单点故障**：在Raft算法中，领导者是负责处理客户端请求和日志复制的节点，如果领导者失效，整个系统的性能和可用性都会受到影响。
3. **数据一致性延迟**：在Raft算法中，当领导者发生变更时，新的领导者需要等待日志复制完成才能处理客户端请求，这可能会导致一定的数据一致性延迟。



#### 算法原理

Raft算法的核心原理包括三个关键组件：领导者选举、日志复制和安全性。

**1. 领导者选举：**

- 每个节点在任意时刻可能处于三种状态之一：领导者（leader）、跟随者（follower）和候选者（candidate）。
- 初始情况下，所有节点都是跟随者。如果一个跟随者在一段时间内没有收到来自领导者的心跳消息，它会转变为候选者并开始选举过程。（无所谓  就是选举 自己的原因也选举）
- 候选者会向其他节点发送投票请求，并在收到多数节点的选票后成为新的领导者。
- 如果在选举过程中出现多个候选者获得相同票数的情况，那么会进行新一轮的选举，直到只有一个候选者获胜。

**2. 日志复制：**

- Raft算法使用日志来记录系统中的所有操作。每个节点都有一个日志，其中包含一系列的日志条目。
- 当客户端向领导者发送写请求时，领导者会将该请求作为一个新的日志条目追加到自己的日志中，并向其他节点发送日志复制请求。
- 其他节点收到复制请求后，会将该日志条目追加到自己的日志中，并向领导者发送确认消息。
- 一旦领导者收到多数节点的确认消息，该日志条目被视为已提交，并将其应用到状态机中执行相应操作。

**3. 安全性：**

- Raft算法通过多数投票机制来确保系统的安全性。任何一条已提交的日志条目都必须在多数节点上复制和执行，才能保证数据的一致性。
- 如果一个节点成为领导者，并开始复制日志条目，但在复制完成之前失去了领导者地位，那么新的领导者将继续复制剩余的日志条目。
- 如果一个节点在复制过程中发现自己的日志与领导者的日志不一致，它将回退到领导者的日志状态，并重新进行复制。

总的来说，Raft算法通过领导者选举、日志复制和安全性机制，实现了分布式系统中的一致性和可靠性。它的设计简单易懂，易于实现，并且提供了强一致性保证。



#### 领导者选举

1. 初始状态下，所有节点都是跟随者（Follower）状态。
2. 如果一个跟随者在一段时间内没有收到来自领导者（Leader）的心跳消息，它会转变为候选者（Candidate）并开始选举过程。
3. 候选者向其他节点发送投票请求，并请求其他节点投票给自己。
4. 其他节点在收到投票请求后，如果还没有投票给其他候选者，且候选者的日志更新且比自己的日志新，就会投票给候选者。
5. 如果候选者收到了多数节点的选票（包括自己的一票），那么它就成为新的领导者。
6. 如果在选举过程中出现多个候选者获得相同票数的情况，那么会进行新一轮的选举，直到只有一个候选者获胜。

通过以上步骤，Raft 算法实现了分布式系统中的领导者选举机制，确保系统能够选出稳定的领导者来协调其他节点的操作。







#### 集群和分布式

集群是将多个服务器实现同一个业务，来实现一项业务的性能提升，分布式是不同的机器实现不同的业务，通过分布式部署来实现系统整体的性能提升，不同的业务可以集群部署



### zookeeper

ZAB协议是保证ZK一致性的原子广播协议
ZK实现一致性共三个阶段:
1.选举leader:因为只有leader节点处理写操作
2.数据同步:所有follower要与leader保持数据一致性
3.请求广播:收到写请求的时候，会将写请求广播到所有follower节点，从而尽量使得所有节点的写操作是同时处理的



为什么ZK可以用来做注册中心

1. ZK可以保存对应的key-value
2. ZK的watch机制可以做服务发现
3. ZK底层用的多线程模型，性能也还行
    ZK可以监控每个服务的连接情况，从而通报给监控的节点，从而更新对应的服务URL



ZK的leader领导选举流程是怎样的

核心是投票选举
所有follower
1.首先投票给自己
2.两两PK，赢家不做操作，输家将票改投赢家，或者赢家的赢家。
3.如果PK是平局，则票还是投给自己
4.PK结束，输家将票数情况传给下一个PK对象
5.如果有节点PK赢过一半以上的节点，则直接被选为leader



ZK节点数据是如何同步的[或者说同步流程有哪些？]

和raft一样的策略s

1.集群启动时，首先选举出leader节点
2.leader节点收到写请求
-------------日志持久化-----------------------
3.leader将日志发送给follower
4.follower日志持久化成功以后，返回ACK确认给leader
---------------内存更新-----------------------
5.leader收到半数以上的ACK以后，leader更新内存数据
6.将commit指令发送给从节点
---------------返回客户端------------------------------
7.leader节点返回响应成功









### 协程

- 对称协程 c++20  各个调用者和协程关系是对等的，协程可以resume到任意一个协程
- 非对称协程 只能yeild到调用者，再由调用者resume到其他
- 有栈协程  协程有自己的栈帧，通过改寄存器ebp和esp来实现协程的调用（栈帧可以在堆区）https://www.bilibili.com/video/BV1y3411u7Me/?spm_id_from=333.337.search-card.all.click&vd_source=3aa535caf6a287f00d045ee5ffb0228d
- 无栈协程 没有自己的栈帧，状态机维护跳转位置
- 无栈协程和有栈协程的区别就是，是否可以在协程中调用的函数中挂起协程，有栈协程有自己的栈帧，当调用函数时会生成被调函数的栈帧，因此可以在任意调用层级进行让出，但是如果层级较多，内存不够用就会出现溢出错误。无栈协程只是在刚开始的时候就把协程需要用到的内存分配好了，并且存入了协程中需要用到状态信息，当发生函数调用时，只能等待其函数返回后才能让出，也就是只能自己挂自己。







#### 协程和io多路复用的结合

b站 https://www.bilibili.com/video/BV1a5411b7aZ/?spm_id_from=333.337.search-card.all.click&vd_source=3aa535caf6a287f00d045ee5ffb0228d

主要是一些需要等待的事件（connext accept read write），用协程做一层包装，因为在io多路复用的时候，当可读或者可写事件没有处理完（数据没读完，对方服务器没有发完，数据没写完，缓冲区没有空间0），需要重复注册监听事件时，协程函数就把事件注册了，然后让出（这里就不一定让给调用者了（协程调度器），可以给另一个协程来处理同类事件）





### 分布式图床介绍

项目利用多个中间件实现的一个分布式图床项目，其中使用nginx做反向代理和负载均衡，mysql数据库来存储关系型数据，redis来存储用户的token，使用fastDFS来充当图床的分布式存储引擎，使用jsoncpp来对post内容的json串进行解析，用log4cplus来实现日志的记录，并用llhttp来对http请求的解析，实现了一个分布式的图床系统，主要有一下几个功能，分别是注册、登录、文件妙传、文件共享、文件下载排行榜。（proactor模型，主线程读取数据，业务交给子线程处理）这里当用户前端发起请求时，请求会发送到nginx层，nginx对请求进行转发给后端处理程序，后端根据不同的请求进行不同的业务处理，处理完请求就把响应数据通过nginx回发给客户端，

#### 压测

单线程有索引随机插入10000条数据qbs约1304 随机查询1000次qbs约6993 无索引随机插入10000条数据qbs约1318 随机查询1000次qbs约150

#### 技术选型

这里存储引擎选用的fastDFS，首先是因为我这里做的是图床，一般都是一些小文件，fastDFS在小文件存储上有很大的优势，有专门的小文件存储策略，而ceph比较适用于大文件存储，在大文件上比较有优势，而且fastDFS天生支持分布式，使得扩展性得到了保证，其次就是选用redis来存储一些热点内容，这里主要考虑到redis有丰富的数据结构，使得对数据的操作变得非常容易，比如排行榜功能，还有就是redis带有持久化功能，项目中的token数据是可以持久化到磁盘中的，即使服务器宕机了，还是可以恢复的，还有就是redis的分布式功能了，如若分享的文件变多，使用redis可以很简单的扩展。选用nginx是nginx是高性能的http服务器，支持反向代理和负载均衡，在后期如果并发量增加，可以使用nginx实现应用层负载均衡，(使用llhttp来做http的解析，主要考虑到了llhttp的高性能和社区比较活跃，http_parse已经停止维护了。)

#### 功能实现

- 文件秒传：原理就是上传的时候先向服务器发送文件的md5，msyql有一张file_info的表，这张表中存储着已上传文件的一些信息，其中包含了文件的md5值，如果中存在此文件的md5值，那么只需copy一下mysql表中的数据即可实现文件秒传，如果不存在那么就继续上传即可。
- 图片分享：mysql中有一个share_picture_list表，其中存储的是图片的md5，和一个url标识符，这个标识符是随机生成的，我们如果想要访问分享的图片就可以在url中加上这个图片的标识符，然后后端就会根据标识符来联合查询file_info表得到图片的真实url。
- token验证：利用redis中的hset来实现用户token的存储，第一次成功登录后，会生成一个token返回给客户端，然后在redis中对token进行存储，用户名为key，token就是value，之后的每次访问都使用token来做验证
- 下载排行榜：使用redis的zset数据结构，实现了根据共享文件的下载量实现排行榜
- 文件上传，使用了nginx-upload模块，用户提交post表单，nginx把文件存储到临时目录上，然后把文件元数据发送给后端程序，后端程序把文件上传到fastDFS服务器上面，然后删除临时文件
- 文件下载：这里使用了fastDFS的扩展模块，每个storage节点上部署nginx，由nginx来提供下载文件的功能，根据上传的文件id来获取文件，如果文件不在当前节点，则向源storage发起重定向或者代理

#### 项目优化

1. 我在一些频繁使用where查询的mysql字段做了索引来优化查询功能，因为文件的查询，插入都需要操作表file_info中的文件md5，因此对md5进行建立前缀索引，来优化查询；还会根据用户名来查询密码，因此在user_file中对用户名建立索引，文件上传的时候会根据user_file_list中的用户名 md5和文件名唯一确定一条记录，因此对这三个列建立联合索引，等等。  
2. 并对一些热点数据用redis做了一层缓存，来提升性能，因此每个连接都会分配一个token，登录后token全程携带，因此用redis来缓存，文件数量缓存到了redis，因为如若用户文件很多，计算文件数量非常耗时，因此使用redis来进行缓存。
3. 我查看了fastDFS上传客户端的代码，发现其在tracker的选择上只是简单使用了轮询的方式，我在其基础上对其进行优化，使用了一致性hash算法来实现tracker的选择，完成了负载均衡

#### 项目难点

- 首先就是在跟这个项目初期的时候，很难在全局上把握项目，因为涉及的中间件和相关技术较多，一些上层业务的处理也不熟悉，很难能一下子理解代码的用意，并且由于第一个webServer项目也比较简单没有复杂的业务和中间件，基本全是自己在造轮子，因此面对这个项目很难能一下子消化完，我采取的办法就是分模块去学习去理解，比如项目用到的中间件，我会先去了解一下这些中间件以及如何使用和应用场景，然后再回到源码上进行阅读，针对那些上层业务我也是把业务进行划分，一个模块一个模块的进行学习，画出思维导图，分析每一个业务需要用到的中间件，以及如何使用的，把这些小模块全部学习完毕，再从全局上进行分析，全部分析就不再纠结细节，只考虑其功能。
- 还有就是一些第三方开源库的学习上，这个项目使用到了两个第三方开源库，分别是log4cplus和llhttp，我在log4cplus上的学习上，感觉有点困难，因为我去学习新的东西，我首先会在搜索引擎上搜索，看看有没有前辈已经有好的文档了，如果有了，那么就省很多事情了，但是关于这个库我搜到的资料都是不明所以，很零碎无法学习，因此我只能去官方的github上去阅读它的example，例子倒是很多的，但是没有注释，导致我阅读起来很困难，我只能一句一句的阅读、调试来理解语句的作用和目的。最后通过自己的学习也差不多能懂如何使用了。

#### fastDFS架构

- 客户端：
  - 上传文件：先向tarcker进行通信，tracker返回storage的信息，然后客户端把文件上传到storage节点
  - 下载文件：先向tracker进行通信，tracker返回要下载文件所在的storage节点的信息，客户端进行下载
- Tracker 跟踪器集群
  - 主要是是维护storage集群的信息，这里storage会定期发送心跳包给tracker，其中包括storage的信息
  - 这里每个tracker节点都是对等的，客户端访问任何一个tracker都可行，多个tracker节点避免了单点故障问题，实现系统的高可用性
- Storage集群
  - Storage集群按照组进行划分，组与组之间是横向扩容关系，来提升集群的容量
  - 组内是纵向扩容，用来做数据冗余备份，避免单点故障，保证高可用性

#### 小文件存储（Trunk文件机制）

首先使能小文件存储功能，后续的文件小于16M就使用trunk文件存储，使用trunk文件（默认64M）存储生成的文件id就发生了变化，id中加入了trunk文件的文件id和小文件的在trunk文件的偏移，小文件在trunk中使用trunkHeader（描述了文件信息）+文件内容来存储，Storage内部会使用我实现的内存池一样来管理空闲存储空间，不过不同大小链表之间是使用平衡树组织的，当无剩余时，重新构建一个trunk文件。

#### fastDFS 组内同步策略

这里同步简单粗暴，各个每个storage都会和每个tracker建立连接，同步一些元数据，只有同组的storage需要复制同步文件，简单粗暴，tracker会记录所有stoarge的信息，组之间的storage不需要通信，tracker之间会进行通信，而且也会有leader，但是leader的作用和普通的tracker是一模一样的，就是对等的，所以是没有作用的

这里用到了binlog，每个storage节点都有一个binlog，里面记录了文件的信息，同步过程中就是用到这个binlog，storage是不存有group内其他storage的信息的，是通过向tracker发送心跳包收到的回复中得到的，对组内每一个storage开启一个同步线程进行同步，并且对组内每一个storage有一个mark文件，标记同步进度。这里有一个最小同步时间，会定期跟tracker汇报，tracker根据这个时间判断storage中是否有存在某个文件

#### 负载均衡策略

##### nginx(多reactor多进程)

- 轮询策略（默认负载均衡策略）
- 最少连接数负载均衡策略（惊群现象自己处理，上锁，谁取到锁谁就accept，因为要实现负载均衡的粗略，系统处理惊群就不能负载均衡了，linux系统其实已经做好了关于accept的惊群现象的避免，但是如果使用linux那么不能实现负载均衡）
- ip-hash 负载均衡策略
- 权重负载均衡策略，这个是依概率实现的  一致性hash（许你节点个数实现权重轮询）

##### fastDFS

- tracker
  - 这里所有的tracker是对等的，可以选择任意一个tracker（这里查看了上传程序的代码，发现上传程序选择tracker的规则是简单的轮询，自己在其基础上实现一致性hash算法，实现tracker选择的负载均衡，使用文件名作为key）
  - **因为没有固定的选择策略，因此这里可以引入一致性hash**
    - 这里采用主机的ip:port作为key，经过对key拼接一些随机信息其计算crc32值，得到一个32位无符号值，存入c++的map中，每次查找，就把要查找的文件名做key进行计算crc32值，然后在map中寻找其下一个节点即可。
- group
  - Round robin，所有的group间轮询
  - Specified group，指定某一个确定的group
  - Load balance，选择最大剩余空 间的组上传文件
- storage
  - Round robin，在group内的所有storage间轮询。
  - First server ordered by ip，按IP排序，也会轮询。
  - First server ordered by priority，按优先级排序（优先级在storage上配置）；可以理解为权重的方式，权重高的优先选择。比如，storage 1 为100M的带宽和storage 2为500M带宽，上传文件时优先使用500M带宽的storage。
- storage path
  - Round robin，多个存储目录间轮询。
  - 剩余存储空间最多的优先。

####  cookie session jwt(token)

cookie是客户端存储的内容用户名

session是服务端存储的内容 客户端有个session的id用来取服务端的状态信息

jwt不需要存储，签名验证即可



#### 和对象存储的区别（ceph）

- 首先这个fastDFS的分布式存储引擎，属于一个文件系统，使用目录树来组织文件的，用户需要根据具体的目录和文件名来获取文件，就和普通的文件系统无异
- 而对象存储是近几年才出现的技术，对象存储没有目录级别，直接根据文件唯一id就可以获取文件，把文件当作一个对象存储在服务器中。
- 采用文件系统的存储引擎存储数据是一个层级的结构，而对象存储是扁平化的结构
